{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXMhXU4nsp7Z"
   },
   "source": [
    "# Hand Gesture CNN model creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Institution: Carleton University\n",
    "# Course: OSS4009 Capstone \n",
    "# Term: F22 - W23\n",
    "#\n",
    "# Filename: 0 - Pre2 - cnn_model_creation.ipynb\n",
    "#\n",
    "# Students: Adam Thompson, Philippe Beaulieu\n",
    "# Advisor:  Dr. Marzieh Amini\n",
    "#\n",
    "# Description: This program will create the model from images in a folder structure,\n",
    "#              and trained, the model can be converted to a Tensorflow light and saved.\n",
    "#              You can test the mode with an image or live stream with a webcam in\n",
    "#              the bottom code sections\n",
    "#              You can save the model to a Tensorflow Light Model after testing the\n",
    "#              training and validation\n",
    "#\n",
    "#              This program will only look at the DEPTH images for the Tensorflow model.\n",
    "#              The folder hierarchy is important to load the images, it is as follow:\n",
    "#\n",
    "#     TRAIN\n",
    "#       -DEPTH\n",
    "#          -folder0\n",
    "#             - image0.jpg\n",
    "#             - image1.jpg\n",
    "#             - image2.jpg\n",
    "#             - ...\n",
    "#          -folder1\n",
    "#             - image0.jpg\n",
    "#             - image1.jpg\n",
    "#             - image2.jpg\n",
    "#             - ...\n",
    "#          -...\n",
    "#       -RGB\n",
    "#          -folder0\n",
    "#             - image0.jpg\n",
    "#             - image1.jpg\n",
    "#             - image2.jpg\n",
    "#             - ...\n",
    "#          -folder1\n",
    "#             - image0.jpg\n",
    "#             - image1.jpg\n",
    "#             - image2.jpg\n",
    "#             - ...\n",
    "#          -...\n",
    "#     TEST -> follow the same structure as train\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5395,
     "status": "ok",
     "timestamp": 1675805532379,
     "user": {
      "displayName": "P Beaulieu (TGClerics)",
      "userId": "08279420019805274378"
     },
     "user_tz": 300
    },
    "id": "iaf3X-nVsp7a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJ0ywv5gsp7b"
   },
   "source": [
    "setting up the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3264,
     "status": "ok",
     "timestamp": 1675805657174,
     "user": {
      "displayName": "P Beaulieu (TGClerics)",
      "userId": "08279420019805274378"
     },
     "user_tz": 300
    },
    "id": "q-GjC3rUsp7c",
    "outputId": "ca9e6e36-6a0d-411f-c6d7-5f20cc2b784c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n"
     ]
    }
   ],
   "source": [
    "#data_dir = pathlib.Path(archive).with_suffix('')\n",
    "data_dir = pathlib.Path('TRAIN/DEPTH').with_suffix('')\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcytsup9sp7d"
   },
   "source": [
    "Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1629,
     "status": "ok",
     "timestamp": 1675805682736,
     "user": {
      "displayName": "P Beaulieu (TGClerics)",
      "userId": "08279420019805274378"
     },
     "user_tz": 300
    },
    "id": "tE_yweMesp7e",
    "outputId": "388a5abd-466f-40b3-87ee-91f034de5bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4500 files belonging to 5 classes.\n",
      "Using 3600 files for training.\n",
      "Found 4500 files belonging to 5 classes.\n",
      "Using 900 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "#img_height = 270\n",
    "#img_width  = 480\n",
    "img_height = 120\n",
    "img_width  = 160\n",
    "\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  labels='inferred',\n",
    "  color_mode='rgb',\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=27,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  labels='inferred',\n",
    "  color_mode='rgb',\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=27,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1675805691972,
     "user": {
      "displayName": "P Beaulieu (TGClerics)",
      "userId": "08279420019805274378"
     },
     "user_tz": 300
    },
    "id": "FyiZ8D_gsp7e",
    "outputId": "9559883c-dd11-4295-9039-4edb71091a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1finger', '2finger', 'fist', 'palm', 'thumb_up']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPzIwlqasp7f"
   },
   "source": [
    "Visualize the data (no need to be run unless you are curious of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "executionInfo": {
     "elapsed": 92622,
     "status": "error",
     "timestamp": 1675805786942,
     "user": {
      "displayName": "P Beaulieu (TGClerics)",
      "userId": "08279420019805274378"
     },
     "user_tz": 300
    },
    "id": "-ff3Mb_Ksp7g",
    "outputId": "0f558894-baf0-439c-a775-bf3ce80351ac"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kK66_o4Isp7h",
    "outputId": "0e4ff3b2-b0bf-46e1-8f45-a0694e8751ee"
   },
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrIrDJHmsp7i"
   },
   "source": [
    "Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rkd-5FzUsp7k"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "#train_ds = train_ds.cache().shuffle(1024).prefetch(buffer_size=AUTOTUNE)\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model - network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O_wYuyoOsp7k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),     # Normalize the input data\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n",
    "    #layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, 3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Conv2D(128, 3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n",
    "    #layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(256, 3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Conv2D(256, 3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n",
    "    #layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(512, 3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Conv2D(512, 3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n",
    "    #layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5, input_shape=(2,)),   \n",
    "    layers.Dense(256, activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Dense(64, activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Dense(32, activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Dropout(0.3, input_shape=(2,)),   \n",
    "    layers.Dense(num_classes, activation ='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "m-vX5lXxsp7k",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 120, 160, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 120, 160, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 120, 160, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 60, 80, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 80, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 60, 80, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 40, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 30, 40, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 30, 40, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 15, 20, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 15, 20, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 15, 20, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 10, 512)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 35840)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 35840)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               9175296   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,904,069\n",
      "Trainable params: 13,904,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DumYKZCsp7l",
    "outputId": "5d3810d5-84f9-4381-85a0-b6e114860523",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "113/113 [==============================] - 345s 3s/step - loss: 1.4019 - accuracy: 0.3894 - val_loss: 0.7458 - val_accuracy: 0.6767\n",
      "Epoch 2/35\n",
      "113/113 [==============================] - 331s 3s/step - loss: 0.7214 - accuracy: 0.6931 - val_loss: 0.4157 - val_accuracy: 0.8400\n",
      "Epoch 3/35\n",
      "113/113 [==============================] - 335s 3s/step - loss: 0.4255 - accuracy: 0.8503 - val_loss: 0.1517 - val_accuracy: 0.9567\n",
      "Epoch 4/35\n",
      "113/113 [==============================] - 336s 3s/step - loss: 0.2485 - accuracy: 0.9219 - val_loss: 0.1162 - val_accuracy: 0.9600\n",
      "Epoch 5/35\n",
      "113/113 [==============================] - 338s 3s/step - loss: 0.1645 - accuracy: 0.9500 - val_loss: 0.0566 - val_accuracy: 0.9811\n",
      "Epoch 6/35\n",
      "113/113 [==============================] - 328s 3s/step - loss: 0.0862 - accuracy: 0.9744 - val_loss: 0.0489 - val_accuracy: 0.9867\n",
      "Epoch 7/35\n",
      "113/113 [==============================] - 330s 3s/step - loss: 0.1115 - accuracy: 0.9675 - val_loss: 0.1507 - val_accuracy: 0.9622\n",
      "Epoch 8/35\n",
      "113/113 [==============================] - 333s 3s/step - loss: 0.0912 - accuracy: 0.9717 - val_loss: 0.0474 - val_accuracy: 0.9867\n",
      "Epoch 9/35\n",
      "113/113 [==============================] - 328s 3s/step - loss: 0.0747 - accuracy: 0.9822 - val_loss: 0.0321 - val_accuracy: 0.9900\n",
      "Epoch 10/35\n",
      "113/113 [==============================] - 345s 3s/step - loss: 0.0782 - accuracy: 0.9753 - val_loss: 0.0829 - val_accuracy: 0.9800\n",
      "Epoch 11/35\n",
      "113/113 [==============================] - 324s 3s/step - loss: 0.0614 - accuracy: 0.9869 - val_loss: 0.0465 - val_accuracy: 0.9889\n",
      "Epoch 12/35\n",
      "113/113 [==============================] - 330s 3s/step - loss: 0.0673 - accuracy: 0.9803 - val_loss: 0.0363 - val_accuracy: 0.9889\n",
      "Epoch 13/35\n",
      "113/113 [==============================] - 335s 3s/step - loss: 0.0607 - accuracy: 0.9828 - val_loss: 0.0278 - val_accuracy: 0.9911\n",
      "Epoch 14/35\n",
      "113/113 [==============================] - 323s 3s/step - loss: 0.0460 - accuracy: 0.9883 - val_loss: 0.0108 - val_accuracy: 0.9956\n",
      "Epoch 15/35\n",
      "113/113 [==============================] - 344s 3s/step - loss: 0.0472 - accuracy: 0.9864 - val_loss: 0.0040 - val_accuracy: 0.9989\n",
      "Epoch 16/35\n",
      "113/113 [==============================] - 349s 3s/step - loss: 0.0389 - accuracy: 0.9908 - val_loss: 7.5513e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/35\n",
      "113/113 [==============================] - 341s 3s/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0043 - val_accuracy: 0.9978\n",
      "Epoch 18/35\n",
      "113/113 [==============================] - 343s 3s/step - loss: 0.0033 - accuracy: 0.9983 - val_loss: 1.5384e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/35\n",
      "113/113 [==============================] - 341s 3s/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 3.2159e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/35\n",
      "113/113 [==============================] - 330s 3s/step - loss: 0.0475 - accuracy: 0.9892 - val_loss: 0.0243 - val_accuracy: 0.9956\n",
      "Epoch 21/35\n",
      "113/113 [==============================] - 340s 3s/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.0790 - val_accuracy: 0.9856\n",
      "Epoch 22/35\n",
      "113/113 [==============================] - 333s 3s/step - loss: 0.0225 - accuracy: 0.9942 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 23/35\n",
      " 51/113 [============>.................] - ETA: 2:57 - loss: 0.0666 - accuracy: 0.9841"
     ]
    }
   ],
   "source": [
    "epochs=35\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#img = tf.keras.utils.load_img('TEST/RGB/2finger/frame596.jpg', target_size=(img_height, img_width))\n",
    "img = tf.keras.utils.load_img('TEST/DEPTH/test1.png', target_size=(img_height, img_width))\n",
    "#img = tf.keras.utils.load_img('TRAIN/DEPTH/1finger/img333.jpg', target_size=(img_height, img_width))\n",
    "\n",
    "# preparing the image for prediction\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "# running the prediction on the image array\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "# display the image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# print the prediction result\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score)) )\n",
    "print(predictions)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Keras Sequential model to a TensorFlow Lite model - and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "# Print the signatures from the converted model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "signatures  = interpreter.get_signature_list()\n",
    "print(signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1LdfWmTW94XzUdzRYniqwusvHlX1YVLN2",
     "timestamp": 1643382579159
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
