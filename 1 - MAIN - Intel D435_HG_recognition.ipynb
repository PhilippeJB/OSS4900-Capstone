{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3cfee2",
   "metadata": {
    "id": "dc3cfee2"
   },
   "source": [
    "# Intel D435 prediction cam codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe828b",
   "metadata": {
    "id": "8fbe828b"
   },
   "outputs": [],
   "source": [
    "# Institution: Carleton University\n",
    "# Course: OSS4900 Capstone \n",
    "# Term: F22 - W23\n",
    "#\n",
    "# Filename: 1 - MAIN - Intel D435_HG_recognition.ipynb\n",
    "#\n",
    "# Students: Adam Thompson, Philippe Beaulieu\n",
    "# Advisor:  Dr. Marzieh Amini\n",
    "#\n",
    "# Description: This program load the Mediapipe .csv dataset to be used with the RGB stream, and\n",
    "#              load the light CNN model to be used with the DEPTH stream, if the DEPTH prediction\n",
    "#              falls under the threshold (here 60%) the mediapipe hand model result will be used.\n",
    "#\n",
    "#              In this project we use Mediapipe as support when the CNN model goes below a set threshold.\n",
    "#     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472b8fb",
   "metadata": {
    "id": "5472b8fb"
   },
   "source": [
    "setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf4055",
   "metadata": {
    "id": "01bf4055"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3e89a",
   "metadata": {
    "id": "1fb3e89a"
   },
   "source": [
    "Load the trainedf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d730ad",
   "metadata": {
    "id": "43d730ad",
    "outputId": "ef79156e-3873-4944-c8d3-2463b14c9769"
   },
   "outputs": [],
   "source": [
    "TF_MODEL_FILE_PATH = 'model.tflite' # The default path to the saved TensorFlow Lite model\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)\n",
    "\n",
    "interpreter.allocate_tensors()         # Needed before execution!\n",
    "interpreter.get_signature_list()\n",
    "\n",
    "signatures = interpreter.get_signature_list()\n",
    "print(signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cfc39",
   "metadata": {
    "id": "9a7cfc39",
    "outputId": "de624dd4-f233-4451-b23b-9f2c3dfec110"
   },
   "outputs": [],
   "source": [
    "mp_hands          = mp.solutions.hands\n",
    "mp_drawing        = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "data    = pd.read_csv(\"dataset3.csv\",index_col=0) \n",
    "X,Y     = data.iloc[:,:63],data['target']\n",
    "\n",
    "#{using SVC}\n",
    "#model   = SVC(kernel = 'rbf')\n",
    "\n",
    "#{using K Neighbors Classifier}  - good standing and sitting\n",
    "model   = KNeighborsClassifier() # Initialize our classifier\n",
    "model.fit(X,Y)\n",
    "\n",
    "with open(\"class_name.json\", 'r') as f:\n",
    "    class_name = json.load(f)\n",
    "\n",
    "print(class_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7201c",
   "metadata": {
    "id": "c7f7201c"
   },
   "source": [
    "Using Intel D435 pipeline input for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b5fee",
   "metadata": {
    "id": "ce1b5fee"
   },
   "outputs": [],
   "source": [
    "# Redefining class name for controls\n",
    "class_name = ['Volume Up', 'Volume Down', 'Previous Song', 'Play / Pause', 'Next Song']\n",
    "\n",
    "# Label variable\n",
    "asan  = \"\"\n",
    "asan2 = \"\"\n",
    "\n",
    "#img_height = 270\n",
    "#img_width = 480\n",
    "img_height = 120\n",
    "img_width  = 160\n",
    "\n",
    "# Colors.\n",
    "blue        = (255, 127, 0)\n",
    "green       = (127, 255, 0)\n",
    "dark_blue   = (127, 20, 0)\n",
    "light_green = (127, 233, 100)\n",
    "yellow      = (0, 255, 255)\n",
    "pink        = (255, 0, 255)\n",
    "red         = (50, 50, 255)\n",
    "colors = green\n",
    "color2 = colors\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"This code requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    \n",
    "    with mp_hands.Hands(max_num_hands=1,\n",
    "                        model_complexity=1, \n",
    "                        min_detection_confidence=0.6, \n",
    "                        min_tracking_confidence=0.6) as hands:\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            # Wait for a coherent pair of frames: depth and color\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            depth_frame = frames.get_depth_frame()\n",
    "            color_frame = frames.get_color_frame()\n",
    "            if not depth_frame or not color_frame:\n",
    "                continue\n",
    "\n",
    "            decimation = rs.decimation_filter()\n",
    "            decimation.set_option(rs.option.filter_magnitude, 1)\n",
    "            decimated_depth = decimation.process(depth_frame)\n",
    "\n",
    "            spatial = rs.spatial_filter()\n",
    "            spatial.set_option(rs.option.filter_magnitude, 5)\n",
    "            spatial.set_option(rs.option.filter_smooth_alpha, 1)\n",
    "            spatial.set_option(rs.option.filter_smooth_delta, 50)\n",
    "            filtered_depth = spatial.process(decimated_depth)    \n",
    "                   \n",
    "            hole_filling = rs.hole_filling_filter()\n",
    "            filled_depth = hole_filling.process(depth_frame)\n",
    "                \n",
    "            #preparing the stream captured images\n",
    "            # Convert images to numpy arrays\n",
    "            depth_image = np.asanyarray(filled_depth.get_data())\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "            # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.15), cv2.COLORMAP_JET)\n",
    "\n",
    "            depth_colormap_dim = depth_colormap.shape\n",
    "            color_colormap_dim = color_image.shape\n",
    "\n",
    "\n",
    "            \n",
    "            # CNN prediction processing section    \n",
    "            # processing image for prediction    \n",
    "            img_array = tf.keras.utils.img_to_array(cv2.resize(depth_colormap, (img_height, img_width)))\n",
    "            #img_array = tf.keras.utils.img_to_array(cv2.resize(images, (img_height, img_width)))\n",
    "            img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "            classify_lite = interpreter.get_signature_runner('serving_default')\n",
    "# l input   #predictions_lite = classify_lite(sequential_1_input=input)['outputs']\n",
    "            predictions_lite = classify_lite(rescaling_input=img_array)['dense_4']\n",
    "            score_lite = tf.nn.softmax(predictions_lite)\n",
    "                \n",
    "            # set the color of the text for action predicted\n",
    "            if (np.argmax(score_lite) == 0):\n",
    "                colors = green\n",
    "            elif (np.argmax(score_lite) == 1):\n",
    "                colors = light_green\n",
    "            elif (np.argmax(score_lite) == 2):\n",
    "                colors = yellow\n",
    "            elif (np.argmax(score_lite) == 3):\n",
    "                colors = pink\n",
    "            else:\n",
    "                colors = red \n",
    "\n",
    "            # print the class on the image\n",
    "            #print(\"{} at {:.2f}%\".format(class_name[np.argmax(score_lite)], 100 * np.max(score_lite)))\n",
    "            asan = 'CNN ' + class_name[np.argmax(score_lite)]     # extract the class name to be displayed            \n",
    "\n",
    "                \n",
    "            # Mediapipe KNN prediction processing section              \n",
    "            results = hands.process(color_image)\n",
    "            ttemp   = []\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(color_image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                              mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                                              mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "                    for point in mp_hands.HandLandmark:\n",
    "                        nLandmark = hand_landmarks.landmark[point]\n",
    "                        ttemp = ttemp + [nLandmark.x, nLandmark.y, nLandmark.z]\n",
    "\n",
    "                # KNN prediction\n",
    "                preds = model.predict([ttemp])\n",
    "                asan2 = 'KNN ' + class_name[int(preds)]\n",
    "                if (int(preds) == 0):\n",
    "                    color2 = green\n",
    "                elif (int(preds) == 1):\n",
    "                    color2 = light_green\n",
    "                elif (int(preds) == 2):\n",
    "                    color2 = yellow\n",
    "                elif (int(preds) == 3):\n",
    "                    color2 = pink\n",
    "                else:\n",
    "                    color2 = red \n",
    "\n",
    "                #cv2.putText(color_image, asan2, (50,50), cv2.FONT_HERSHEY_SIMPLEX,1,color2,3)\n",
    "\n",
    "\n",
    "            # for a single extimation result, use the codes below\n",
    "            if ((100 * np.max(score_lite)) < 60):\n",
    "                asan = asan2\n",
    "                colors = color2\n",
    "\n",
    "            #cv2.putText(depth_colormap, asan, (50,50), cv2.FONT_HERSHEY_SIMPLEX,1,colors,3)            \n",
    "            #cv2.putText(images, asan, (50,50), cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),3)\n",
    "            #cv2.putText(color_image, asan, (400,50), cv2.FONT_HERSHEY_SIMPLEX,1,colors,3)\n",
    "            cv2.putText(color_image, asan, (300,50), cv2.FONT_HERSHEY_SIMPLEX,1,colors,3)\n",
    "            \n",
    "            # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "            if depth_colormap_dim != color_colormap_dim:\n",
    "                resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "                images = np.hstack((resized_color_image, depth_colormap))\n",
    "            else:\n",
    "                images = np.hstack((color_image, depth_colormap))            \n",
    "\n",
    "            \n",
    "            # Show images \n",
    "            cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.imshow('RealSense', images)\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key & 0xFF == ord('q') or key == 27:\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5f185",
   "metadata": {
    "id": "bca5f185"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67618261",
   "metadata": {
    "id": "67618261"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
